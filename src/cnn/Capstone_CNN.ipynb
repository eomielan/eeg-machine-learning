{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwy-DbzIpzzq",
        "outputId": "846a3908-059d-43f9-e2c3-93b71c56ab79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "# Takes the SHFT images that are generated in file \"Capstone_CNN_SHFT_CREATION\",the images are sorted in the ZIP file under test, train and validate.\n",
        "# The SHFTs are created using the raw data columns in the raw dataset folder.\n",
        "# Purpose of this model is to take the images as inputs into the binary CNN classification model.\n",
        "# The output of this model is the accuracy of being able to predict whether a participant is doing a problem solving task or a rcall task.\n",
        "# Note many parameters of this model were tweaked to increase the accuracy such as the split of test data and number of epochs.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# This code was created on google collab and requires access to the files/images which were stored on the drive.\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uMWfQ-ZdcUBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Augmentation and Sourcing the Images"
      ],
      "metadata": {
        "id": "rL-1LKXwYaki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   validation_split=0.3) # Specify 20% for validation\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/ManuCapstone/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary',\n",
        "                                                 subset='training')  # Specify that this is the training set\n",
        "validation_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/ManuCapstone/train',\n",
        "                                                   target_size=(64, 64),\n",
        "                                                   batch_size=32,\n",
        "                                                   class_mode='binary',\n",
        "                                                   subset='validation')  # Specify that this is the validation set"
      ],
      "metadata": {
        "id": "jxiZZqegqF8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd9b16a-1e81-48bf-b478-965f42ef8289"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 images belonging to 2 classes.\n",
            "Found 6 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sourcing the Test Folder Which Contains the Images\n"
      ],
      "metadata": {
        "id": "faimarJDYiEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/ManuCapstone/test',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "metadata": {
        "id": "FWDeeroDs9By",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ce8a58-4e1a-4823-fcc4-3f596fc203ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 81 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Creation"
      ],
      "metadata": {
        "id": "9a5A3z7zYqKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = tf.keras.models.Sequential()\n",
        "Model.add(tf.keras.layers.Conv2D(filters=4, kernel_size=5, activation='relu', input_shape=[64, 64, 3])) #First layer, relu activation was used along with 4 filiters.\n",
        "Model.add(tf.keras.layers.MaxPool2D(pool_size=4, strides=2))\n",
        "Model.add(tf.keras.layers.Flatten())\n",
        "Model.add(tf.keras.layers.Dense(units=16, activation='tanh'))# First dense layer containing 16 neurons.\n",
        "Model.add(tf.keras.layers.Dense(units=16, activation='relu'))# Second dense layer containing 16 neurons.\n",
        "Model.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) # 1 neruron for the last layer to find the final class for this binary classification.\n",
        "Model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "s0neeSlQs_uJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=Model.fit(x = training_set, validation_data = validation_set, epochs = 20) #20 epochs yielded the same results as using 50, 80, 100. For time sake 20 was used."
      ],
      "metadata": {
        "id": "hEVcGH4GtFTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ef1190-87d7-44e4-b46e-fb6e3875ad07"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6888 - accuracy: 0.5000 - val_loss: 0.8148 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 1s 643ms/step - loss: 0.7863 - accuracy: 0.5000 - val_loss: 0.7325 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 1s 642ms/step - loss: 0.7017 - accuracy: 0.5000 - val_loss: 0.7101 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 0.6900 - accuracy: 0.5000 - val_loss: 0.7255 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 1s 770ms/step - loss: 0.7007 - accuracy: 0.5000 - val_loss: 0.7225 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 1s 632ms/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.7140 - val_accuracy: 0.3333\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 1s 628ms/step - loss: 0.6866 - accuracy: 0.5000 - val_loss: 0.7184 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 0.6855 - accuracy: 0.5714 - val_loss: 0.7235 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 0.6829 - accuracy: 0.5714 - val_loss: 0.7291 - val_accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.6789 - accuracy: 0.5714 - val_loss: 0.7345 - val_accuracy: 0.1667\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 0.6778 - accuracy: 0.5714 - val_loss: 0.7404 - val_accuracy: 0.1667\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.6768 - accuracy: 0.6429 - val_loss: 0.7509 - val_accuracy: 0.1667\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 0.6842 - accuracy: 0.6429 - val_loss: 0.7708 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 0.6787 - accuracy: 0.6429 - val_loss: 0.7700 - val_accuracy: 0.1667\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 446ms/step - loss: 0.6707 - accuracy: 0.6429 - val_loss: 0.7675 - val_accuracy: 0.1667\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 0.6663 - accuracy: 0.5714 - val_loss: 0.7862 - val_accuracy: 0.1667\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 0.6715 - accuracy: 0.5000 - val_loss: 0.7997 - val_accuracy: 0.1667\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.6681 - accuracy: 0.5714 - val_loss: 0.7970 - val_accuracy: 0.1667\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 0.6587 - accuracy: 0.5714 - val_loss: 0.8206 - val_accuracy: 0.1667\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 0.6632 - accuracy: 0.6429 - val_loss: 0.8058 - val_accuracy: 0.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vaildating for Select Images for Debugging Purposes"
      ],
      "metadata": {
        "id": "1rLEWNT_ZiSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "path = r'/content/drive/MyDrive/Colab Notebooks/ManuCapstone/Validate' #File path for the select images for internal testing to help with debugging.\n",
        "for img in os.listdir(path):\n",
        "#Iterates over the list of files in the specified directory\n",
        "  test_image = image.load_img(path+\"/\"+img, target_size = (64, 64))\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  predict = Model.predict(test_image)\n",
        "  if predict[0][0] > 0.5:\n",
        "      prediction = 'Recall'\n",
        "  else :\n",
        "      prediction = 'Maze'\n",
        "  print(img + \" = \"+ prediction)"
      ],
      "metadata": {
        "id": "oadYlnextH9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baefa772-d949-4d01-bd28-2a5f6c9b1326"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 75ms/step\n",
            "Copy of Copy of spectrogram_JasmineMaze_RAW_TP10_maze.png = Maze\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Copy of Copy of spectrogram_JasmineRecall_RAW_TP9_recall.png = Maze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Meteric of the Model In Terms of Accuracy -> ~58%"
      ],
      "metadata": {
        "id": "9mzbsXF_ZpS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = Model.evaluate(test_set)\n",
        "print(f'Test Accuracy: {test_accuracy[1]}')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "true_labels = [] #Making empty vectors\n",
        "predicted_labels = []\n",
        "\n",
        "for i in range(test_set.n // test_set.batch_size + 1): # Had issues cause of the batch size. This helped overcome that.\n",
        "    batch_data, batch_labels = test_set.next()\n",
        "    true_labels.extend(batch_labels)\n",
        "    batch_predictions = Model.predict(batch_data)\n",
        "    predicted_labels.extend(np.round(batch_predictions).astype(int))\n",
        "\n",
        "\n",
        "confusion_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "uZIU6h4rtxmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e572d94-646d-4647-d668-0b7c5a0aa2b5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 405ms/step - loss: 0.6906 - accuracy: 0.5556\n",
            "Test Accuracy: 0.5555555820465088\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "[[24 19]\n",
            " [17 21]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model had a average slightly over 58%, however it flucated around ~5%. With more images this model would become much more accurate. Also by more filtering and increasing the number of pixels in the model may yield better accuarcy."
      ],
      "metadata": {
        "id": "YwT5eg9acVsv"
      }
    }
  ]
}